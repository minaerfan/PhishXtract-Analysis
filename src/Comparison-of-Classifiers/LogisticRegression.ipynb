{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d629124-35bd-426a-8144-aad076c758a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T16:40:17.500597Z",
     "start_time": "2025-02-08T16:40:16.608957Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def domain_age_lessThanOne(report, create_date, update_date):\n",
    "    if create_date != \"\" and create_date != \"expired\" and not pd.isna(create_date):\n",
    "        age = datetime.strptime(report[:10], '%Y-%m-%d') - datetime.strptime(create_date[:10], '%Y-%m-%d')\n",
    "        return (age.days // 365) < 1\n",
    "    elif create_date == \"\" and update_date != \"\":\n",
    "        age = datetime.strptime(report[:10], '%Y-%m-%d') - datetime.strptime(update_date[:10], '%Y-%m-%d')\n",
    "        if age.days < 365:\n",
    "            return None\n",
    "        else:\n",
    "            return False\n",
    "    elif create_date == \"expired\":\n",
    "        return True\n",
    "    return None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-08T16:40:19.175503Z",
     "start_time": "2025-02-08T16:40:19.168441Z"
    }
   },
   "id": "a92da31a22061903",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def binary_to_numeric(value):\n",
    "    if value:\n",
    "        return 1\n",
    "    if not value:\n",
    "        return 0\n",
    "    else:\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-08T16:40:19.543178Z",
     "start_time": "2025-02-08T16:40:19.526711Z"
    }
   },
   "id": "f79c650a5e232ba0",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ff2a1f7-e937-4acd-bfcc-7f1b144846ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T16:40:19.788324Z",
     "start_time": "2025-02-08T16:40:19.770755Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_data(data, features):\n",
    "    \n",
    "    preprocessed_data = data[features].copy()\n",
    "    preprocessed_data['new_domain'] = None\n",
    "    report_date = \"2024-04-23\"\n",
    "    for index, item in preprocessed_data.iterrows():\n",
    "        new_domain = domain_age_lessThanOne(report_date, item['creation_date'], item['updated_date'])\n",
    "        preprocessed_data.loc[index, 'new_domain'] = new_domain\n",
    "        \n",
    "    preprocessed_data = preprocessed_data.drop('creation_date', axis=1)\n",
    "    preprocessed_data = preprocessed_data.drop('updated_date', axis=1)\n",
    "        \n",
    "    # Transform binary values to numerical\n",
    "    preprocessed_data['control_over_dns'] = preprocessed_data['control_over_dns'].astype(float).replace({True: 1.0, False: 0.0})\n",
    "    preprocessed_data['domain_indexed'] = preprocessed_data['domain_indexed'].astype(float).replace({True: 1.0, False: 0.0})\n",
    "    preprocessed_data['is_archived'] = preprocessed_data['is_archived'].astype(float).replace({True: 1.0, False: 0.0})\n",
    "    preprocessed_data['known_hosting'] = preprocessed_data['known_hosting'].astype(float).replace({True: 1.0, False: 0.0})\n",
    "    preprocessed_data['new_domain'] = preprocessed_data['new_domain'].astype(float).replace({True: 1.0, False: 0.0})\n",
    "    preprocessed_data['is_on_root'] = preprocessed_data['is_on_root'].astype(float).replace({True: 1.0, False: 0.0})\n",
    "    preprocessed_data['is_subdomain'] = preprocessed_data['is_subdomain'].astype(float).replace({True: 1.0, False: 0.0})\n",
    "        \n",
    "    return preprocessed_data"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "\n",
    "\n",
    "def train_imputers(X_train, numerical_features):\n",
    "    features_with_missing = X_train.columns[X_train.isnull().any()].tolist()\n",
    "    features_with_missing.sort(key=lambda x: X_train[x].isnull().sum())\n",
    "\n",
    "    trained_imputers = {}\n",
    "    best_params_dict = {}\n",
    "\n",
    "    for feature in features_with_missing:\n",
    "        complete_train = X_train.dropna(subset=[feature])\n",
    "        \n",
    "        param_grid = {\n",
    "            \"n_estimators\": [100, 200, 500],\n",
    "            \"max_depth\": [None, 5, 10],\n",
    "            \"min_samples_split\": [2, 5, 10]\n",
    "        }\n",
    "        \n",
    "        if feature in numerical_features:\n",
    "            model = RandomForestRegressor(random_state=0)\n",
    "            scoring = 'neg_mean_squared_error'\n",
    "        else:\n",
    "            model = RandomForestClassifier(random_state=0)\n",
    "            scoring = 'accuracy'\n",
    "        \n",
    "        X_train_feat = complete_train.drop(feature, axis=1)\n",
    "        y_train_feat = complete_train[feature]\n",
    "\n",
    "        grid_search = GridSearchCV(model, param_grid, cv=5, scoring=scoring, n_jobs=-1)\n",
    "        grid_search.fit(X_train_feat, y_train_feat)\n",
    "        \n",
    "        best_model = grid_search.best_estimator_\n",
    "        best_params_dict[feature] = grid_search.best_params_\n",
    "        \n",
    "        # Store the trained imputer model for this feature\n",
    "        trained_imputers[feature] = best_model\n",
    "\n",
    "        # Impute missing values in the training set itself\n",
    "        X_train_null = X_train[X_train[feature].isnull()].drop(feature, axis=1)\n",
    "        if len(X_train_null) > 0:\n",
    "            imputed_values = best_model.predict(X_train_null)\n",
    "            X_train.loc[X_train[feature].isnull(), feature] = imputed_values\n",
    "\n",
    "    return X_train, trained_imputers\n",
    "\n",
    "\n",
    "def apply_imputers(X_test, trained_imputers):\n",
    "    for feature, imputer_model in trained_imputers.items():\n",
    "        X_test_null = X_test[X_test[feature].isnull()].drop(feature, axis=1)\n",
    "        if len(X_test_null) > 0:\n",
    "            imputed_values = imputer_model.predict(X_test_null)\n",
    "            X_test.loc[X_test[feature].isnull(), feature] = imputed_values\n",
    "    return X_test\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-08T16:40:20.106961Z",
     "start_time": "2025-02-08T16:40:20.015981Z"
    }
   },
   "id": "f7732c46b82b822f",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def perform_classification(data, labels, sample_ids, path_prefix):\n",
    "    # Map labels to numeric values\n",
    "    label_mapping = {'attackers_domain': 0, 'compromised_domain': 1, 'shared_domain': 2}\n",
    "    y = labels.map(label_mapping)\n",
    "    X = data.copy()\n",
    "    \n",
    "    numerical_features = ['between_archives_distance', 'phish_archives_distance']\n",
    "    \n",
    "    scaler_max_abs_list = []\n",
    "    \n",
    "    param_grid = [\n",
    "    {\n",
    "        'penalty': ['l2'],\n",
    "        'C': [0.1, 1, 10],\n",
    "        'solver': ['lbfgs', 'sag', 'newton-cg'],\n",
    "        'max_iter': [1000, 2000]\n",
    "    },\n",
    "    {\n",
    "        'penalty': ['l1'],\n",
    "        'C': [0.1, 1, 10],\n",
    "        'solver': ['liblinear', 'saga'],\n",
    "        'max_iter': [1000, 2000]\n",
    "    },\n",
    "    {\n",
    "        'penalty': ['elasticnet'],\n",
    "        'C': [0.1, 1, 10],\n",
    "        'solver': ['saga'],\n",
    "        'l1_ratio': [0.5],\n",
    "        'max_iter': [1000, 2000]\n",
    "    },\n",
    "    {\n",
    "        'penalty': [None],  \n",
    "        'solver': ['lbfgs', 'sag', 'newton-cg', 'saga'],  \n",
    "        'max_iter': [1000, 2000]\n",
    "    }\n",
    "]\n",
    "    \n",
    "    model_to_tune = LogisticRegression()\n",
    "    \n",
    "    # Declare the inner and outer cross-validation strategies\n",
    "    inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    \n",
    "    outer_confusion_matrices = []\n",
    "    outer_precision_list = []\n",
    "    outer_recall_list = []\n",
    "    outer_f1_list = []\n",
    "    y_true_list = []\n",
    "    y_pred_list = []\n",
    "    sample_id_list = []\n",
    "    fold_data_list = []\n",
    "    best_params_list = []\n",
    "    \n",
    "    for i, (outer_train_index, outer_test_index) in enumerate(outer_cv.split(X, y)):\n",
    "        X_outer_train = X.iloc[outer_train_index].reset_index(drop=True)\n",
    "        X_outer_test = X.iloc[outer_test_index].reset_index(drop=True)\n",
    "        y_outer_train = y.iloc[outer_train_index].reset_index(drop=True)\n",
    "        y_outer_test = y.iloc[outer_test_index].reset_index(drop=True)\n",
    "        sample_ids_outer_test = sample_ids.iloc[outer_test_index].reset_index(drop=True)\n",
    "    \n",
    "        # Fit MaxAbsScaler on X_outer_train[numerical_features]\n",
    "        scaler = MaxAbsScaler()\n",
    "        scaler.fit(X_outer_train[numerical_features])\n",
    "\n",
    "        scaler_max_abs_list.append(scaler.max_abs_)\n",
    "\n",
    "        X_outer_train_scaled = X_outer_train.copy()\n",
    "        X_outer_test_scaled = X_outer_test.copy()\n",
    "        X_outer_train_scaled[numerical_features] = scaler.transform(\n",
    "            X_outer_train[numerical_features]\n",
    "        )\n",
    "        X_outer_test_scaled[numerical_features] = scaler.transform(\n",
    "            X_outer_test[numerical_features]\n",
    "        )\n",
    "        \n",
    "        # Handle missing values (if any) in training set and get trained imputers\n",
    "        X_outer_train_scaled_imputed, trained_imputers = train_imputers(X_outer_train_scaled, numerical_features)\n",
    "        X_outer_test_scaled_imputed = apply_imputers(X_outer_test_scaled, trained_imputers)\n",
    "        \n",
    "        # Inner cross-validation for parameter search on the current outer fold\n",
    "        model = GridSearchCV(\n",
    "            estimator=model_to_tune, param_grid=param_grid, cv=inner_cv, n_jobs=-1, scoring=\"f1_macro\"\n",
    "        )\n",
    "        model.fit(X_outer_train_scaled_imputed, y_outer_train)\n",
    "    \n",
    "        best_params_list.append(model.best_params_)\n",
    "    \n",
    "        y_pred = model.predict(X_outer_test_scaled_imputed)\n",
    "    \n",
    "        confusion_matrix_values = confusion_matrix(y_outer_test, y_pred)\n",
    "        outer_confusion_matrices.append(confusion_matrix_values)\n",
    "    \n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            y_outer_test, y_pred, average=None, labels=[0, 1, 2]\n",
    "        )\n",
    "        outer_precision_list.append(precision)\n",
    "        outer_recall_list.append(recall)\n",
    "        outer_f1_list.append(f1)\n",
    "    \n",
    "        y_true_list.extend(y_outer_test)\n",
    "        y_pred_list.extend(y_pred)\n",
    "        sample_id_list.extend(sample_ids_outer_test)\n",
    "    \n",
    "        print(f\"Outer Fold {i+1} Confusion Matrix:\\n{outer_confusion_matrices[-1]}\")\n",
    "        for j, (p, r, f_val) in enumerate(zip(precision, recall, f1)):\n",
    "            print(\n",
    "                f\"Outer Fold {i+1} Class {j} Precision: {p:.3f}, Recall: {r:.3f}, \"\n",
    "                f\"F1-score: {f_val:.3f}\"\n",
    "            )\n",
    "    \n",
    "        fold_data = X_outer_test.copy()\n",
    "        fold_data['sample_id'] = sample_ids_outer_test\n",
    "        fold_data['actual'] = y_outer_test.map({v: k for k, v in label_mapping.items()})\n",
    "        fold_data['predicted'] = pd.Series(y_pred).map(\n",
    "            {v: k for k, v in label_mapping.items()}\n",
    "        )\n",
    "\n",
    "        fold_data_list.append(fold_data)\n",
    "    \n",
    "    fold_data_all = pd.concat(fold_data_list, axis=0).reset_index(drop=True)\n",
    "    fold_data_all.to_csv(f\"{path_prefix}logistic_regression_predictions_all.csv\", index=True)\n",
    "    \n",
    "    # Aggregate best parameters from each fold\n",
    "    for params in best_params_list:\n",
    "        if 'l1_ratio' in params and params['penalty'] != 'elasticnet':\n",
    "            del params['l1_ratio']\n",
    "    \n",
    "    best_params_df = pd.DataFrame(best_params_list)\n",
    "    # Choose the parameters that appear most frequently\n",
    "    best_params = best_params_df.mode().iloc[0].to_dict()\n",
    "    \n",
    "    param_types = {\n",
    "        'penalty': str,\n",
    "        'C': float,\n",
    "        'solver': str,\n",
    "        'max_iter': int,\n",
    "        'multi_class': str,\n",
    "        'l1_ratio': float  # l1_ratio is only used when penalty='elasticnet'\n",
    "    }\n",
    "    \n",
    "    for param, param_type in param_types.items():\n",
    "        if param in best_params:\n",
    "            if param == 'l1_ratio' and best_params['penalty'] != 'elasticnet':\n",
    "                continue\n",
    "            best_params[param] = param_type(best_params[param])\n",
    "    \n",
    "    # Aggregate max absolute values from all folds\n",
    "    scaler_max_abs_array = np.array(scaler_max_abs_list)\n",
    "    aggregated_max_abs = np.mean(scaler_max_abs_array, axis=0)\n",
    "\n",
    "    max_abs_df = pd.DataFrame({\n",
    "        'feature': numerical_features,\n",
    "        'max_abs': aggregated_max_abs\n",
    "    })\n",
    "    max_abs_df.to_csv(f\"{path_prefix}scaler_max_abs_values.csv\", index=False)\n",
    "\n",
    "    # Fit final scaler on entire dataset for future use\n",
    "    final_scaler = MaxAbsScaler()\n",
    "    final_scaler.fit(X[numerical_features])\n",
    "    \n",
    "    joblib.dump(final_scaler, f\"{path_prefix}scaler.pkl\")\n",
    "    \n",
    "    # Transform the entire dataset\n",
    "    X_scaled = X.copy()\n",
    "    X_scaled[numerical_features] = final_scaler.transform(X[numerical_features])\n",
    "    \n",
    "    X_scaled_imputed, trained_imputers_whole = train_imputers(X_scaled, numerical_features)\n",
    "\n",
    "    # Retrain the final model on the entire dataset using the best hyperparameters\n",
    "    model_final = LogisticRegression(**best_params)\n",
    "    model_final.fit(X_scaled_imputed, y)\n",
    "    \n",
    "    joblib.dump(model_final, f\"{path_prefix}logistic_regression_model.pkl\")\n",
    "    \n",
    "    model_params = model_final.get_params()\n",
    "    with open(f\"{path_prefix}logistic_regression_model_params.txt\", 'w') as f:\n",
    "        for param, value in model_params.items():\n",
    "            f.write(f\"{param}: {value}\\n\")\n",
    "    \n",
    "    average_precision = np.mean(outer_precision_list, axis=0)\n",
    "    average_recall = np.mean(outer_recall_list, axis=0)\n",
    "    average_f1 = np.mean(outer_f1_list, axis=0)\n",
    "    \n",
    "    std_precision = np.std(outer_precision_list, axis=0)\n",
    "    std_recall = np.std(outer_recall_list, axis=0)\n",
    "    std_f1 = np.std(outer_f1_list, axis=0)\n",
    "\n",
    "    print(\"\\nAverage and Standard Deviation of Precision, Recall, and F1-score Across All Folds:\")\n",
    "    for j in range(len(average_precision)):\n",
    "        print(\n",
    "            f\"Class {j} - Precision: {average_precision[j]:.3f} ± {std_precision[j]:.3f}, \"\n",
    "            f\"Recall: {average_recall[j]:.3f} ± {std_recall[j]:.3f}, \"\n",
    "            f\"F1-score: {average_f1[j]:.3f} ± {std_f1[j]:.3f}\"\n",
    "        )\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-08T17:04:32.176828Z",
     "start_time": "2025-02-08T17:04:32.168414Z"
    }
   },
   "id": "1b62131c33cc72cf",
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d8fd297-f2db-4588-b200-a1688013e92b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T17:04:32.423132Z",
     "start_time": "2025-02-08T17:04:32.408865Z"
    }
   },
   "outputs": [],
   "source": [
    "# List of selected features\n",
    "selected_features = [\n",
    "    'creation_date',\n",
    "    'updated_date',\n",
    "    'control_over_dns',\n",
    "    'domain_indexed',\n",
    "    'known_hosting',\n",
    "    'is_archived',\n",
    "    'is_on_root',\n",
    "    'is_subdomain',\n",
    "    'between_archives_distance',\n",
    "    'phish_archives_distance'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6629d049-b414-431a-ab84-2ce3606d6cac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T17:04:32.693326Z",
     "start_time": "2025-02-08T17:04:32.681473Z"
    }
   },
   "outputs": [],
   "source": [
    "numerical_features = ['between_archives_distance', 'phish_archives_distance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "130a5bf5-d4dc-4c84-b64a-aeed112ef9c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T17:04:32.907541Z",
     "start_time": "2025-02-08T17:04:32.888486Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "path_prefix = '../../results/'\n",
    "df = pd.read_csv('../../PhishXtract-Class/Phish-Xtract-Class-Labeled/validated_dataset_for_classification.csv')\n",
    "target = df['verified_category']\n",
    "ids = df['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d00114fe-3a52-4044-a5d5-0cee666dd444",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T17:04:33.556959Z",
     "start_time": "2025-02-08T17:04:33.180180Z"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "transformed_data = preprocess_data(df, selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array(['attackers_domain', 'compromised_domain', 'shared_domain'],\n",
      "      dtype=object), array([1377,  106, 3953]))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(np.unique(target, return_counts=True))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-08T17:04:33.557458Z",
     "start_time": "2025-02-08T17:04:33.550614Z"
    }
   },
   "id": "f7428810-b14c-489c-a6fa-e4ad660673f8",
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer Fold 1 Confusion Matrix:\n",
      "[[261   2  13]\n",
      " [  2   3  16]\n",
      " [  3   2 786]]\n",
      "Outer Fold 1 Class 0 Precision: 0.981, Recall: 0.946, F1-score: 0.963\n",
      "Outer Fold 1 Class 1 Precision: 0.429, Recall: 0.143, F1-score: 0.214\n",
      "Outer Fold 1 Class 2 Precision: 0.964, Recall: 0.994, F1-score: 0.979\n",
      "Outer Fold 2 Confusion Matrix:\n",
      "[[265   1  10]\n",
      " [  4   2  15]\n",
      " [  3   2 785]]\n",
      "Outer Fold 2 Class 0 Precision: 0.974, Recall: 0.960, F1-score: 0.967\n",
      "Outer Fold 2 Class 1 Precision: 0.400, Recall: 0.095, F1-score: 0.154\n",
      "Outer Fold 2 Class 2 Precision: 0.969, Recall: 0.994, F1-score: 0.981\n",
      "Outer Fold 3 Confusion Matrix:\n",
      "[[265   1   9]\n",
      " [  2   2  18]\n",
      " [  2   0 788]]\n",
      "Outer Fold 3 Class 0 Precision: 0.985, Recall: 0.964, F1-score: 0.974\n",
      "Outer Fold 3 Class 1 Precision: 0.667, Recall: 0.091, F1-score: 0.160\n",
      "Outer Fold 3 Class 2 Precision: 0.967, Recall: 0.997, F1-score: 0.982\n",
      "Outer Fold 4 Confusion Matrix:\n",
      "[[266   0   9]\n",
      " [  1   3  17]\n",
      " [  5   3 783]]\n",
      "Outer Fold 4 Class 0 Precision: 0.978, Recall: 0.967, F1-score: 0.973\n",
      "Outer Fold 4 Class 1 Precision: 0.500, Recall: 0.143, F1-score: 0.222\n",
      "Outer Fold 4 Class 2 Precision: 0.968, Recall: 0.990, F1-score: 0.979\n",
      "Outer Fold 5 Confusion Matrix:\n",
      "[[266   0   9]\n",
      " [  5   2  14]\n",
      " [  3   1 787]]\n",
      "Outer Fold 5 Class 0 Precision: 0.971, Recall: 0.967, F1-score: 0.969\n",
      "Outer Fold 5 Class 1 Precision: 0.667, Recall: 0.095, F1-score: 0.167\n",
      "Outer Fold 5 Class 2 Precision: 0.972, Recall: 0.995, F1-score: 0.983\n",
      "\n",
      "Average and Standard Deviation of Precision, Recall, and F1-score Across All Folds:\n",
      "Class 0 - Precision: 0.978 ± 0.005, Recall: 0.961 ± 0.008, F1-score: 0.969 ± 0.004\n",
      "Class 1 - Precision: 0.532 ± 0.114, Recall: 0.113 ± 0.024, F1-score: 0.183 ± 0.029\n",
      "Class 2 - Precision: 0.968 ± 0.002, Recall: 0.994 ± 0.002, F1-score: 0.981 ± 0.002\n"
     ]
    }
   ],
   "source": [
    "perform_classification(transformed_data, target, ids, path_prefix)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-08T17:05:24.571916Z",
     "start_time": "2025-02-08T17:04:33.911925Z"
    }
   },
   "id": "ca464098bf1fd65e",
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ea7f3f3811c915c1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
