{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d629124-35bd-426a-8144-aad076c758a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T13:10:32.113319Z",
     "start_time": "2024-12-11T13:10:31.105914Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, make_scorer, accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def domain_age_lessThanOne(report, create_date, update_date):\n",
    "    if create_date != \"\" and create_date != \"expired\" and not pd.isna(create_date):\n",
    "        age = datetime.strptime(report[:10], '%Y-%m-%d') - datetime.strptime(create_date[:10], '%Y-%m-%d')\n",
    "        return (age.days // 365) < 1\n",
    "    elif create_date == \"\" and update_date != \"\":\n",
    "        age = datetime.strptime(report[:10], '%Y-%m-%d') - datetime.strptime(update_date[:10], '%Y-%m-%d')\n",
    "        if age.days < 365:\n",
    "            return None\n",
    "        else:\n",
    "            return False\n",
    "    elif create_date == \"expired\":\n",
    "        return True\n",
    "    return None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-11T13:10:32.179206Z",
     "start_time": "2024-12-11T13:10:32.128191Z"
    }
   },
   "id": "a92da31a22061903",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def binary_to_numeric(value):\n",
    "    if value:\n",
    "        return 1\n",
    "    if not value:\n",
    "        return 0\n",
    "    else:\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-11T13:10:32.187567Z",
     "start_time": "2024-12-11T13:10:32.179621Z"
    }
   },
   "id": "f79c650a5e232ba0",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ff2a1f7-e937-4acd-bfcc-7f1b144846ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T13:10:32.195498Z",
     "start_time": "2024-12-11T13:10:32.186929Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_data(data, features):\n",
    "    \n",
    "    preprocessed_data = data[features].copy()\n",
    "    preprocessed_data['new_domain'] = None\n",
    "    report_date = \"2024-04-23\"\n",
    "    for index, item in preprocessed_data.iterrows():\n",
    "        new_domain = domain_age_lessThanOne(report_date, item['creation_date'], item['updated_date'])\n",
    "        preprocessed_data.loc[index, 'new_domain'] = new_domain\n",
    "        \n",
    "    preprocessed_data = preprocessed_data.drop('creation_date', axis=1)\n",
    "    preprocessed_data = preprocessed_data.drop('updated_date', axis=1)\n",
    "        \n",
    "    # Transform binary values to numerical\n",
    "    preprocessed_data['control_over_dns'] = preprocessed_data['control_over_dns'].astype(float).replace({True: 1.0, False: 0.0})\n",
    "    preprocessed_data['domain_indexed'] = preprocessed_data['domain_indexed'].astype(float).replace({True: 1.0, False: 0.0})\n",
    "    preprocessed_data['is_archived'] = preprocessed_data['is_archived'].astype(float).replace({True: 1.0, False: 0.0})\n",
    "    preprocessed_data['known_hosting'] = preprocessed_data['known_hosting'].astype(float).replace({True: 1.0, False: 0.0})\n",
    "    preprocessed_data['new_domain'] = preprocessed_data['new_domain'].astype(float).replace({True: 1.0, False: 0.0})\n",
    "    preprocessed_data['is_on_root'] = preprocessed_data['is_on_root'].astype(float).replace({True: 1.0, False: 0.0})\n",
    "    preprocessed_data['is_subdomain'] = preprocessed_data['is_subdomain'].astype(float).replace({True: 1.0, False: 0.0})\n",
    "        \n",
    "    return preprocessed_data"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "\n",
    "\n",
    "def train_imputers(X_train, numerical_features):\n",
    "    features_with_missing = X_train.columns[X_train.isnull().any()].tolist()\n",
    "    features_with_missing.sort(key=lambda x: X_train[x].isnull().sum())\n",
    "\n",
    "    trained_imputers = {}\n",
    "    best_params_dict = {}\n",
    "\n",
    "    for feature in features_with_missing:\n",
    "        complete_train = X_train.dropna(subset=[feature])\n",
    "        \n",
    "        param_grid = {\n",
    "            \"n_estimators\": [100, 200, 500],\n",
    "            \"max_depth\": [None, 5, 10],\n",
    "            \"min_samples_split\": [2, 5, 10]\n",
    "        }\n",
    "        \n",
    "        if feature in numerical_features:\n",
    "            model = RandomForestRegressor(random_state=0)\n",
    "            scoring = 'neg_mean_squared_error'\n",
    "        else:\n",
    "            model = RandomForestClassifier(random_state=0)\n",
    "            scoring = 'accuracy'\n",
    "        \n",
    "        X_train_feat = complete_train.drop(feature, axis=1)\n",
    "        y_train_feat = complete_train[feature]\n",
    "\n",
    "        grid_search = GridSearchCV(model, param_grid, cv=5, scoring=scoring, n_jobs=-1)\n",
    "        grid_search.fit(X_train_feat, y_train_feat)\n",
    "        \n",
    "        best_model = grid_search.best_estimator_\n",
    "        best_params_dict[feature] = grid_search.best_params_\n",
    "        \n",
    "        # Store the trained imputer model for this feature\n",
    "        trained_imputers[feature] = best_model\n",
    "\n",
    "        # Impute missing values in the training set itself\n",
    "        X_train_null = X_train[X_train[feature].isnull()].drop(feature, axis=1)\n",
    "        if len(X_train_null) > 0:\n",
    "            imputed_values = best_model.predict(X_train_null)\n",
    "            X_train.loc[X_train[feature].isnull(), feature] = imputed_values\n",
    "\n",
    "    return X_train, trained_imputers\n",
    "\n",
    "\n",
    "def apply_imputers(X_test, trained_imputers):\n",
    "    for feature, imputer_model in trained_imputers.items():\n",
    "        X_test_null = X_test[X_test[feature].isnull()].drop(feature, axis=1)\n",
    "        if len(X_test_null) > 0:\n",
    "            imputed_values = imputer_model.predict(X_test_null)\n",
    "            X_test.loc[X_test[feature].isnull(), feature] = imputed_values\n",
    "    return X_test\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "75bcfb07e3a8f196"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def perform_classification(data, labels, sample_ids, path_prefix):\n",
    "    # Map labels to numeric values\n",
    "    label_mapping = {'attackers_domain': 0, 'compromised_domain': 1, 'shared_domain': 2}\n",
    "    y = labels.map(label_mapping)\n",
    "    X = data.copy()\n",
    "    \n",
    "    numerical_features = ['between_archives_distance', 'phish_archives_distance']\n",
    "    \n",
    "    scaler_max_abs_list = []\n",
    "    \n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 500],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'gamma': [0, 0.1, 0.2]\n",
    "    }\n",
    "    model_to_tune = XGBClassifier(eval_metric='mlogloss')\n",
    "    \n",
    "    # Declare the inner and outer cross-validation strategies\n",
    "    inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    \n",
    "    outer_confusion_matrices = []\n",
    "    outer_precision_list = []\n",
    "    outer_recall_list = []\n",
    "    outer_f1_list = []\n",
    "    y_true_list = []\n",
    "    y_pred_list = []\n",
    "    sample_id_list = []\n",
    "    fold_data_list = []\n",
    "    best_params_list = []\n",
    "    \n",
    "    for i, (outer_train_index, outer_test_index) in enumerate(outer_cv.split(X, y)):\n",
    "        X_outer_train = X.iloc[outer_train_index].reset_index(drop=True)\n",
    "        X_outer_test = X.iloc[outer_test_index].reset_index(drop=True)\n",
    "        y_outer_train = y.iloc[outer_train_index].reset_index(drop=True)\n",
    "        y_outer_test = y.iloc[outer_test_index].reset_index(drop=True)\n",
    "        sample_ids_outer_test = sample_ids.iloc[outer_test_index].reset_index(drop=True)\n",
    "    \n",
    "        # Fit MaxAbsScaler on X_outer_train[numerical_features]\n",
    "        scaler = MaxAbsScaler()\n",
    "        scaler.fit(X_outer_train[numerical_features])\n",
    "\n",
    "        scaler_max_abs_list.append(scaler.max_abs_)\n",
    "\n",
    "        X_outer_train_scaled = X_outer_train.copy()\n",
    "        X_outer_test_scaled = X_outer_test.copy()\n",
    "        X_outer_train_scaled[numerical_features] = scaler.transform(\n",
    "            X_outer_train[numerical_features]\n",
    "        )\n",
    "        X_outer_test_scaled[numerical_features] = scaler.transform(\n",
    "            X_outer_test[numerical_features]\n",
    "        )\n",
    "        \n",
    "        # Handle missing values (if any) in training set and get trained imputers\n",
    "        X_outer_train_scaled_imputed, trained_imputers = train_imputers(X_outer_train_scaled, numerical_features)\n",
    "        X_outer_test_scaled_imputed = apply_imputers(X_outer_test_scaled, trained_imputers)\n",
    "    \n",
    "        # Inner cross-validation for parameter search on the current outer fold\n",
    "        model = GridSearchCV(estimator=model_to_tune, param_grid=param_grid, cv=inner_cv, n_jobs=-1, scoring=\"f1_macro\")\n",
    "        model.fit(X_outer_train_scaled_imputed, y_outer_train)\n",
    "    \n",
    "        best_params_list.append(model.best_params_)\n",
    "    \n",
    "        y_pred = model.predict(X_outer_test_scaled_imputed)\n",
    "    \n",
    "        confusion_matrix_values = confusion_matrix(y_outer_test, y_pred)\n",
    "        outer_confusion_matrices.append(confusion_matrix_values)\n",
    "    \n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_outer_test, y_pred, average=None, labels=[0,1,2])\n",
    "        outer_precision_list.append(precision)\n",
    "        outer_recall_list.append(recall)\n",
    "        outer_f1_list.append(f1)\n",
    "    \n",
    "        y_true_list.extend(y_outer_test)\n",
    "        y_pred_list.extend(y_pred)\n",
    "        sample_id_list.extend(sample_ids_outer_test)\n",
    "    \n",
    "        print(f\"Outer Fold {i+1} Confusion Matrix:\\n{outer_confusion_matrices[-1]}\")\n",
    "        for j, (p, r, f_val) in enumerate(zip(precision, recall, f1)):\n",
    "            print(f\"Outer Fold {i+1} Class {j} Precision: {p:.3f}, Recall: {r:.3f}, F1-score: {f_val:.3f}\")\n",
    "    \n",
    "        fold_data = X_outer_test.copy()\n",
    "        fold_data['sample_id'] = sample_ids_outer_test\n",
    "        fold_data['actual'] = y_outer_test.map({v: k for k, v in label_mapping.items()})\n",
    "        fold_data['predicted'] = pd.Series(y_pred).map({v: k for k, v in label_mapping.items()})\n",
    "        fold_data_list.append(fold_data)\n",
    "    \n",
    "    fold_data_all = pd.concat(fold_data_list, axis=0).reset_index(drop=True)\n",
    "    fold_data_all.to_csv(f\"{path_prefix}xgboost_predictions_all.csv\", index=False)\n",
    "    \n",
    "    # Aggregate best parameters from each fold\n",
    "    best_params_df = pd.DataFrame(best_params_list)\n",
    "    # Choose the parameters that appear most frequently\n",
    "    best_params = best_params_df.mode().iloc[0].to_dict()\n",
    "    \n",
    "    param_types = {\n",
    "        'n_estimators': int,\n",
    "        'max_depth': int,\n",
    "        'learning_rate': float,\n",
    "        'subsample': float,\n",
    "        'colsample_bytree': float,\n",
    "        'gamma': float\n",
    "    }\n",
    "    \n",
    "    for param, param_type in param_types.items():\n",
    "        if param in best_params:\n",
    "            best_params[param] = param_type(best_params[param])\n",
    "    \n",
    "    # Aggregate max absolute values from all folds\n",
    "    scaler_max_abs_array = np.array(scaler_max_abs_list)\n",
    "    aggregated_max_abs = np.mean(scaler_max_abs_array, axis=0)\n",
    "\n",
    "    max_abs_df = pd.DataFrame({\n",
    "        'feature': numerical_features,\n",
    "        'max_abs': aggregated_max_abs\n",
    "    })\n",
    "    max_abs_df.to_csv(f\"{path_prefix}scaler_max_abs_values.csv\", index=False)\n",
    "\n",
    "    # Fit final scaler on entire dataset for future use\n",
    "    final_scaler = MaxAbsScaler()\n",
    "    final_scaler.fit(X[numerical_features])\n",
    "\n",
    "    joblib.dump(final_scaler, f\"{path_prefix}scaler.pkl\")\n",
    "\n",
    "    X_scaled = X.copy()\n",
    "    X_scaled[numerical_features] = final_scaler.transform(X[numerical_features])\n",
    "    \n",
    "    X_scaled_imputed, trained_imputers_whole = train_imputers(X_scaled, numerical_features)\n",
    "    \n",
    "    # Retrain the final model on the entire dataset using the best hyperparameters\n",
    "    model_final = XGBClassifier(**best_params, eval_metric='mlogloss')\n",
    "    model_final.fit(X_scaled_imputed, y)\n",
    "    \n",
    "    joblib.dump(model_final, f\"{path_prefix}xgboost_model.pkl\")\n",
    "    \n",
    "    model_params = model_final.get_params()\n",
    "    with open(f\"{path_prefix}xgboost_model_params.txt\", 'w') as f:\n",
    "        for param, value in model_params.items():\n",
    "            f.write(f\"{param}: {value}\\n\")\n",
    "    \n",
    "    average_precision = np.mean(outer_precision_list, axis=0)\n",
    "    average_recall = np.mean(outer_recall_list, axis=0)\n",
    "    average_f1 = np.mean(outer_f1_list, axis=0)\n",
    "    \n",
    "    std_precision = np.std(outer_precision_list, axis=0)\n",
    "    std_recall = np.std(outer_recall_list, axis=0)\n",
    "    std_f1 = np.std(outer_f1_list, axis=0)\n",
    "\n",
    "    print(\"\\nAverage and Standard Deviation of Precision, Recall, and F1-score Across All Folds:\")\n",
    "    for j in range(len(average_precision)):\n",
    "        print(\n",
    "            f\"Class {j} - Precision: {average_precision[j]:.3f} ± {std_precision[j]:.3f}, \"\n",
    "            f\"Recall: {average_recall[j]:.3f} ± {std_recall[j]:.3f}, \"\n",
    "            f\"F1-score: {average_f1[j]:.3f} ± {std_f1[j]:.3f}\"\n",
    "        )\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-11T13:11:50.799056Z",
     "start_time": "2024-12-11T13:11:50.779638Z"
    }
   },
   "id": "1b62131c33cc72cf",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d8fd297-f2db-4588-b200-a1688013e92b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T13:11:51.525183Z",
     "start_time": "2024-12-11T13:11:51.520664Z"
    }
   },
   "outputs": [],
   "source": [
    "# List of selected features\n",
    "selected_features = [\n",
    "    'creation_date',\n",
    "    'updated_date',\n",
    "    'control_over_dns',\n",
    "    'domain_indexed',\n",
    "    'known_hosting',\n",
    "    'is_archived',\n",
    "    'is_on_root',\n",
    "    'is_subdomain',\n",
    "    'between_archives_distance',\n",
    "    'phish_archives_distance'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6629d049-b414-431a-ab84-2ce3606d6cac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T13:11:52.142731Z",
     "start_time": "2024-12-11T13:11:52.125589Z"
    }
   },
   "outputs": [],
   "source": [
    "numerical_features = ['between_archives_distance', 'phish_archives_distance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "130a5bf5-d4dc-4c84-b64a-aeed112ef9c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T13:11:52.842399Z",
     "start_time": "2024-12-11T13:11:52.819448Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "path_prefix = '../../results/'\n",
    "df = pd.read_csv('../../PhishXtract-Class/Phish-Xtract-Class-Labeled/validated_dataset_for_classification.csv')\n",
    "target = df['verified_category']\n",
    "ids = df['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d00114fe-3a52-4044-a5d5-0cee666dd444",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T13:11:54.120414Z",
     "start_time": "2024-12-11T13:11:53.652242Z"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "transformed_data = preprocess_data(df, selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array(['attackers_domain', 'compromised_domain', 'shared_domain'],\n",
      "      dtype=object), array([1376,  106, 3954]))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.unique(target, return_counts=True))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-11T13:12:43.618110Z",
     "start_time": "2024-12-11T13:12:43.605828Z"
    }
   },
   "id": "f7428810-b14c-489c-a6fa-e4ad660673f8",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer Fold 1 Confusion Matrix:\n",
      "[[274   1   1]\n",
      " [  1  18   3]\n",
      " [  5   3 782]]\n",
      "Outer Fold 1 Class 0 Precision: 0.979, Recall: 0.993, F1-score: 0.986\n",
      "Outer Fold 1 Class 1 Precision: 0.818, Recall: 0.818, F1-score: 0.818\n",
      "Outer Fold 1 Class 2 Precision: 0.995, Recall: 0.990, F1-score: 0.992\n",
      "Outer Fold 2 Confusion Matrix:\n",
      "[[268   5   2]\n",
      " [  6  12   3]\n",
      " [  0   2 789]]\n",
      "Outer Fold 2 Class 0 Precision: 0.978, Recall: 0.975, F1-score: 0.976\n",
      "Outer Fold 2 Class 1 Precision: 0.632, Recall: 0.571, F1-score: 0.600\n",
      "Outer Fold 2 Class 2 Precision: 0.994, Recall: 0.997, F1-score: 0.996\n",
      "Outer Fold 3 Confusion Matrix:\n",
      "[[273   1   1]\n",
      " [  4  12   5]\n",
      " [  7   1 783]]\n",
      "Outer Fold 3 Class 0 Precision: 0.961, Recall: 0.993, F1-score: 0.977\n",
      "Outer Fold 3 Class 1 Precision: 0.857, Recall: 0.571, F1-score: 0.686\n",
      "Outer Fold 3 Class 2 Precision: 0.992, Recall: 0.990, F1-score: 0.991\n",
      "Outer Fold 4 Confusion Matrix:\n",
      "[[270   1   4]\n",
      " [  3  13   5]\n",
      " [  2   6 783]]\n",
      "Outer Fold 4 Class 0 Precision: 0.982, Recall: 0.982, F1-score: 0.982\n",
      "Outer Fold 4 Class 1 Precision: 0.650, Recall: 0.619, F1-score: 0.634\n",
      "Outer Fold 4 Class 2 Precision: 0.989, Recall: 0.990, F1-score: 0.989\n",
      "Outer Fold 5 Confusion Matrix:\n",
      "[[272   3   0]\n",
      " [  0  16   5]\n",
      " [  0   0 791]]\n",
      "Outer Fold 5 Class 0 Precision: 1.000, Recall: 0.989, F1-score: 0.995\n",
      "Outer Fold 5 Class 1 Precision: 0.842, Recall: 0.762, F1-score: 0.800\n",
      "Outer Fold 5 Class 2 Precision: 0.994, Recall: 1.000, F1-score: 0.997\n",
      "\n",
      "Average and Standard Deviation of Precision, Recall, and F1-score Across All Folds:\n",
      "Class 0 - Precision: 0.980 ± 0.012, Recall: 0.986 ± 0.007, F1-score: 0.983 ± 0.007\n",
      "Class 1 - Precision: 0.760 ± 0.098, Recall: 0.668 ± 0.102, F1-score: 0.708 ± 0.087\n",
      "Class 2 - Precision: 0.993 ± 0.002, Recall: 0.993 ± 0.004, F1-score: 0.993 ± 0.003\n"
     ]
    }
   ],
   "source": [
    "perform_classification(transformed_data, target, ids, path_prefix)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-11T13:21:25.421201Z",
     "start_time": "2024-12-11T13:12:44.409997Z"
    }
   },
   "id": "ca464098bf1fd65e",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f6edc79d08beee04"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
